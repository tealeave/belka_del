# Belka Transformer Pipeline Configuration
# This file contains all hyperparameters and settings for the molecular transformer pipeline

# Model Architecture Configuration
model:
  depth: 32                    # Hidden dimension size
  num_heads: 8                 # Number of multi-head attention heads
  num_layers: 4                # Number of transformer encoder layers
  dropout_rate: 0.1            # Dropout rate for regularization
  activation: "gelu"           # Activation function (gelu, relu, swish)
  max_length: 128              # Maximum sequence length for padding
  vocab_size: 43               # Vocabulary size (N+2 for PAD and MASK tokens)

# Training Configuration
training:
  batch_size: 2048             # Training batch size
  buffer_size: 10000000        # Shuffle buffer size for dataset
  epochs: 1000                 # Maximum number of training epochs
  patience: 20                 # Early stopping patience
  steps_per_epoch: 10000       # Number of steps per training epoch
  validation_steps: 2000       # Number of validation steps
  masking_rate: 0.15           # MLM masking rate (15% of tokens)
  epsilon: 1e-07               # Small constant for numerical stability

# Data Configuration
data:
  root: "/pub/ddlin/projects/belka_del/data/raw"           # Root directory for raw data files
  working: "/pub/ddlin/projects/belka_del/data/raw"        # Working directory for processed data
  vocab: "/pub/ddlin/projects/belka_del/data/raw/vocab.txt" # Path to vocabulary file

# System Configuration
system:
  seed: 42                     # Random seed for reproducibility
  n_workers: 8                 # Number of parallel workers for data processing

# Advanced Training Options
advanced:
  # Learning rate scheduling
  learning_rate:
    initial: 0.001             # Initial learning rate
    decay_factor: 0.5          # Factor for learning rate decay
    decay_patience: 5          # Patience for learning rate decay
    min_lr: 1e-07              # Minimum learning rate
  
  # Model checkpointing
  checkpointing:
    save_best_only: false      # Save only best models
    save_weights_only: false   # Save only weights (not full model)
    monitor: "loss"            # Metric to monitor for best model
    mode: "min"                # Direction of improvement (min/max)
  
  # Memory optimization
  memory:
    mixed_precision: false     # Use mixed precision training
    gradient_accumulation: 1   # Gradient accumulation steps
    cache_dataset: true        # Cache dataset in memory

# Mode-specific Configurations
modes:
  # Masked Language Model (MLM) pretraining
  mlm:
    description: "Masked language model pretraining on SMILES"
    loss_function: "CategoricalLoss"
    metrics: ["MaskedAUC"]
    use_validation: false      # MLM typically doesn't use validation
    
  # Fingerprint Prediction (FPS)
  fps:
    description: "Molecular fingerprint prediction"
    loss_function: "BinaryLoss"
    metrics: ["MaskedAUC"]
    output_dim: 2048           # ECFP fingerprint dimension
    use_validation: true
    
  # Classification (CLF) for binding affinity
  clf:
    description: "Binding affinity classification"
    loss_function: "MultiLabelLoss" 
    metrics: ["MaskedAUC"]
    output_dim: 3              # Three protein targets: BRD4, HSA, sEH
    use_validation: true
    nan_mask: 2                # Value used for missing labels

# Data Processing Configuration
data_processing:
  # Validation split configuration
  validation:
    method: "building_block_split"  # Split method
    test_size: 0.03                 # Fraction of building blocks for validation
    
  # SMILES processing
  smiles:
    tokenizer: "atomInSmiles"       # Tokenization method
    replace_linker: true            # Replace [Dy] with [H]
    canonicalize: true              # Canonicalize SMILES
    
  # Fingerprint generation
  fingerprints:
    type: "ECFP"                    # Fingerprint type
    radius: 2                       # ECFP radius
    n_bits: 2048                    # Fingerprint length
    include_chirality: true         # Include chirality information
    
  # Memory management
  memory_safe: true                 # Use memory-safe processing by default
  chunk_size: 10000                 # Chunk size for batch processing
  temp_dir: "temp_processing"       # Temporary directory name

# Logging Configuration
logging:
  level: "INFO"                     # Logging level (DEBUG, INFO, WARNING, ERROR)
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: null                        # Log file path (null for console only)
  
# Paths for different environments
environments:
  # Local development
  local:
    data:
      root: "./data/raw"
      working: "./data/processed"
      vocab: "./data/processed/vocab.txt"
      
  # HPC cluster
  hpc:
    data:
      root: "/pub/ddlin/projects/belka_del/data/raw"
      working: "/pub/ddlin/projects/belka_del/data/raw"  
      vocab: "/pub/ddlin/projects/belka_del/data/raw/vocab.txt"
    system:
      n_workers: 16               # More workers on HPC
      
  # GPU cluster  
  gpu:
    training:
      batch_size: 4096            # Larger batch size for GPU
    advanced:
      mixed_precision: true       # Enable mixed precision on GPU

# Competition-specific settings
competition:
  # Kaggle submission format
  submission:
    sample_size: null             # Limit test samples (null for all)
    protein_targets: ["BRD4", "HSA", "sEH"]
    id_column: "id"
    prediction_column: "binds"
    
  # Evaluation metrics
  evaluation:
    primary_metric: "average_precision"
    macro_average: true

# Pipeline execution defaults
pipeline:
  default_mode: "clf"             # Default training mode
  memory_safe_default: true      # Use memory-safe processing by default
  auto_create_dirs: true         # Automatically create missing directories
  checkpoint_recovery: true      # Enable automatic checkpoint recovery
  
# Error handling and recovery
error_handling:
  max_retries: 3                  # Maximum number of retries for failed steps
  retry_delay: 60                 # Delay between retries (seconds)
  fallback_to_cpu: true          # Fallback to CPU if GPU fails
  continue_on_warning: true      # Continue pipeline on warnings